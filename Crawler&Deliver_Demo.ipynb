{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium.webdriver import ChromeOptions\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_list=pd.read_csv('Oceans Companies to Track in Past 90 Days.csv')\n",
    "# company_list=company_list['Name']\n",
    "company_list=pd.read_excel('companyList.xlsx')\n",
    "company_list=company_list['company name']\n",
    "\n",
    "keywords_list=pd.read_excel('keywordList.xlsx')\n",
    "keywords=keywords_list['keywords']\n",
    "\n",
    "columns=keywords.tolist()\n",
    "columns.insert (0, \"Content\")\n",
    "columns.insert (0, \"Link\")\n",
    "columns.insert (0, \"Time\")\n",
    "columns.insert (0, \"Title\")\n",
    "columns.insert (0, \"Source\")\n",
    "\n",
    "# df=pd.read_excel('BusinessWire_'+str(datetime.date.today())+'.xlsx',sheet_name='amazon')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BusinessWire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def businessWire(company,keywords,columns=columns):\n",
    "    path_of_chromedriver = r'./chromedriver' \n",
    "    driver = webdriver.Chrome(path_of_chromedriver)\n",
    "    wait = WebDriverWait(driver=driver, timeout=300)\n",
    "    def search(company):\n",
    "        driver.get('https://www.businesswire.com/portal/site/home/my-business-wire/')\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located((By.ID,'bw-search-input'))\n",
    "        )\n",
    "\n",
    "        search = driver.find_element_by_id('bw-search-input')\n",
    "        search.clear()\n",
    "        search.send_keys(str(company))\n",
    "        search.send_keys(Keys.RETURN)\n",
    "\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME,'bw-news-section'))\n",
    "        )\n",
    "        titles = []\n",
    "        links = []\n",
    "        times = []\n",
    "        for i in range(1,6):\n",
    "            ele = driver.find_element_by_xpath('/html/body/div[1]/div/div/div/div/div[3]/section[2]/ul/li[{}]/h3/a'.format(i))\n",
    "            title = ele.text\n",
    "            titles.append(title)\n",
    "            link = ele.get_attribute('href')\n",
    "            links.append(link)\n",
    "            posted_time = driver.find_element_by_xpath('//*[@id=\"bw-group-all\"]/div/div/div[3]/section[2]/ul/li[{}]/div[1]/time'.format(i)).text\n",
    "            times.append(posted_time)\n",
    "        return(titles, times, links)\n",
    "\n",
    "\n",
    "    def get_content(links):\n",
    "        contents = []\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            content = driver.find_element_by_class_name('bw-release-body  ').text\n",
    "            contents.append(content)\n",
    "    #     print(contents)\n",
    "        return(contents)\n",
    "\n",
    "    def df_transform(df):\n",
    "        for i in keywords:\n",
    "            l=[]\n",
    "            for m in range(len(df.index)):\n",
    "                l2=df.iloc[m]['Content'].split(\"\\n\")\n",
    "                s=\"\"\n",
    "                for z in l2:\n",
    "                    if i in z:\n",
    "                        s+=z\n",
    "                if s==\"\":\n",
    "                    l.append('None')\n",
    "                else:\n",
    "                    l.append(s)\n",
    "\n",
    "            df[i]=l\n",
    "        return df\n",
    "#     writer = pd.ExcelWriter('BusinessWire_'+str(datetime.date.today())+'.xlsx')\n",
    "#     Sheet_list.append('BusinessWire_'+str(datetime.date.today())+'.xlsx')\n",
    "    try:\n",
    "        titles, times, links = search(company)\n",
    "        contents = get_content(links)\n",
    "        source=['BusinessWire'] * len(times)\n",
    "        result_dict = {'Source':source,\n",
    "                    'Title': titles,\n",
    "                   'Time': times,\n",
    "                  'Link': links,\n",
    "                  'Content': contents}\n",
    "        result_pd = pd.DataFrame(result_dict)\n",
    "        df_BusinessWire=df_transform(result_pd)\n",
    "    except:\n",
    "        df_BusinessWire = pd.DataFrame(columns=columns)\n",
    "    driver.quit()\n",
    "\n",
    "    return df_BusinessWire\n",
    "#         writer.save()\n",
    "#         writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prnewswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prnewsWire(company,keywords,columns=columns):\n",
    "    path_of_chromedriver = r'./chromedriver' \n",
    "    driver = webdriver.Chrome(path_of_chromedriver)\n",
    "    wait = WebDriverWait(driver=driver, timeout=300)\n",
    "    def search(company):\n",
    "        driver.get('https://www.prnewswire.com/search/all/?keyword='+str(company))\n",
    "\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME,'row'))\n",
    "        )\n",
    "\n",
    "        titles = []\n",
    "        links = []\n",
    "        times = []\n",
    "\n",
    "        items = driver.find_elements_by_class_name('news-release')\n",
    "        item_num = len(items)\n",
    "        if item_num == 0:\n",
    "            titles = ['Nothing Found']\n",
    "            return(titles, links)\n",
    "        else:\n",
    "            for item in items:\n",
    "                title = item.text\n",
    "                titles.append(title)\n",
    "\n",
    "                link = item.get_attribute('href')\n",
    "                links.append(link)\n",
    "        return(titles, links)\n",
    "\n",
    "    def df_transform(df):\n",
    "        for i in keywords:\n",
    "            l=[]\n",
    "            for m in range(len(df.index)):\n",
    "                l2=df.iloc[m]['Content'].split(\"\\n\")\n",
    "                s=\"\"\n",
    "                for z in l2:\n",
    "                    if i in z:\n",
    "                        s+=z\n",
    "                if s==\"\":\n",
    "                    l.append('None')\n",
    "                else:\n",
    "                    l.append(s)\n",
    "\n",
    "            df[i]=l\n",
    "        return df\n",
    "    def get_content(links):\n",
    "        contents = []\n",
    "        times = []\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            time = driver.find_element_by_class_name('mb-no').text\n",
    "            times.append(time)\n",
    "            content = driver.find_element_by_xpath('//*[@id=\"main\"]/article/section/div/div').text\n",
    "            contents.append(content)\n",
    "        return(times, contents)\n",
    "    try:\n",
    "        company=company.replace(' ','%20')\n",
    "        titles, links = search(company)\n",
    "        times, contents = get_content(links)\n",
    "        source=['Prnewswire']*len(times)\n",
    "        result_dict = {'Source':source,\n",
    "                    'Title': titles,\n",
    "                    'Time': times,\n",
    "                    'Link': links,\n",
    "                    'Content': contents}\n",
    "        df_Prnewswire = pd.DataFrame(result_dict)\n",
    "        df_Prnewswire=df_transform(df_Prnewswire)\n",
    "        company=company.replace('%20',' ')\n",
    "        driver.quit()\n",
    "    except:\n",
    "        df_Prnewswire = pd.DataFrame(columns=columns)\n",
    "    return df_Prnewswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Hunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productHunt(company,keywords,columns=columns):\n",
    "    path_of_chromedriver = r'./chromedriver' \n",
    "    driver = webdriver.Chrome(path_of_chromedriver)\n",
    "    wait = WebDriverWait(driver=driver, timeout=300)\n",
    "    def search(company):\n",
    "        driver.get('https://www.producthunt.com/search?q='+str(company))\n",
    "        titles = []\n",
    "        links = []\n",
    "        for index in range(1,11):\n",
    "            try:\n",
    "                ele = driver.find_element_by_xpath(f'//*[@id=\"__next\"]/div[2]/main/div[2]/div/div/div[{index}]/div[1]/div/h3/a[1]')\n",
    "            except:\n",
    "                continue\n",
    "            title = ele.text\n",
    "            titles.append(title)\n",
    "            link = ele.get_attribute('href')\n",
    "            links.append(link)\n",
    "        return(titles, links)\n",
    "\n",
    "\n",
    "    def get_content(links):\n",
    "        contents = []\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            try:\n",
    "                content = driver.find_element_by_xpath('//*[@id=\"__next\"]/main/div[2]/div/section/div/div[1]/div/div/p').text\n",
    "                contents.append(content)\n",
    "            except:\n",
    "                contents.append('')\n",
    "    #     print(contents)\n",
    "        return(contents)\n",
    "    def df_transform(df):\n",
    "        for i in keywords:\n",
    "            l=[]\n",
    "            for m in range(len(df.index)):\n",
    "                l2=df.iloc[m]['Content'].split(\"\\n\")\n",
    "                s=\"\"\n",
    "                for z in l2:\n",
    "                    if i in z:\n",
    "                        s+=z\n",
    "                if s==\"\":\n",
    "                    l.append('None')\n",
    "                else:\n",
    "                    l.append(s)\n",
    "\n",
    "            df[i]=l\n",
    "        return df\n",
    "    try:\n",
    "        search(company)\n",
    "        titles, links = search(company)\n",
    "        contents = get_content(links)\n",
    "        source=['ProductHunt']*len(titles)\n",
    "        times=[str(datetime.date.today())]*len(titles)\n",
    "        result_dict = {'Source':source,\n",
    "                      'Title': titles,\n",
    "                      'Time':times,\n",
    "                      'Link': links,\n",
    "                      'Content': contents}\n",
    "        df_ProductHunt = pd.DataFrame(result_dict)\n",
    "        df_ProductHunt=df_transform(df_ProductHunt)\n",
    "        driver.quit()\n",
    "    except:\n",
    "        df_ProductHunt = pd.DataFrame(columns=columns)\n",
    "    return df_ProductHunt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venturebeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ventureBeat(company,keywords,columns=columns):\n",
    "    path_of_chromedriver = r'./chromedriver' \n",
    "    driver = webdriver.Chrome(path_of_chromedriver)\n",
    "    wait = WebDriverWait(driver=driver, timeout=300)\n",
    "    def search(company):\n",
    "        driver.get('https://venturebeat.com/?s='+str(company))\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME,'page-title'))\n",
    "        )\n",
    "\n",
    "        titles = []\n",
    "        links = []\n",
    "        times = []\n",
    "\n",
    "        items = driver.find_elements_by_class_name('ArticleListing__body')\n",
    "        item_num = len(items)\n",
    "        if item_num == 0:\n",
    "            return('Nothing found')\n",
    "        else:\n",
    "            i = 1\n",
    "            while i < 7:\n",
    "                ele = driver.find_element_by_xpath('/html/body/div[4]/div/div/div/article[{}]/header/h2/a'.format(i))\n",
    "                title = ele.text.strip()\n",
    "                titles.append(title)\n",
    "                link = ele.get_attribute('href')\n",
    "                links.append(link)\n",
    "                posted_time = driver.find_element_by_xpath('/html/body/div[4]/div/div/div/article[{}]/header/div[2]/time'.format(i)).text\n",
    "                times.append(posted_time)\n",
    "                i+=1\n",
    "            return(titles, times, links)\n",
    "\n",
    "    def get_content(links):\n",
    "        contents = []\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            content = driver.find_element_by_class_name('article-content').text\n",
    "            modified_content = []\n",
    "            parsed_content = content.split('\\n')\n",
    "            length = len(parsed_content)-1\n",
    "            for i in range(1,length):\n",
    "                c = parsed_content[i]\n",
    "                modified_content.append(c)\n",
    "            content = \"\\n\".join(modified_content)\n",
    "            contents.append(content)\n",
    "        return(contents)\n",
    "    def df_transform(df):\n",
    "        for i in keywords:\n",
    "            l=[]\n",
    "            for m in range(len(df.index)):\n",
    "                l2=df.iloc[m]['Content'].split(\"\\n\")\n",
    "                s=\"\"\n",
    "                for z in l2:\n",
    "                    if i in z:\n",
    "                        s+=z\n",
    "                if s==\"\":\n",
    "                    l.append('None')\n",
    "                else:\n",
    "                    l.append(s)\n",
    "\n",
    "            df[i]=l\n",
    "        return df\n",
    "    try:\n",
    "        search(company)\n",
    "        titles, times, links = search(company)\n",
    "        contents = get_content(links)\n",
    "        source=['Venturebeat']*len(titles)\n",
    "        result_dict = {'Source':source,\n",
    "                    'Title': titles,\n",
    "                   'Time': times,\n",
    "                  'Link': links,\n",
    "                  'Content': contents}\n",
    "        df_Venturebeat = pd.DataFrame(result_dict)\n",
    "        driver.quit()\n",
    "    except:\n",
    "        df_Venturebeat = pd.DataFrame(columns=columns)\n",
    "    return df_Venturebeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('Target Company Info-'+str(datetime.date.today())+'.xlsx')\n",
    "for company in company_list:\n",
    "    if len(company)>=30:\n",
    "        company=company[:30]\n",
    "    df1=businessWire(company,keywords)\n",
    "    df2=prnewsWire(company,keywords)\n",
    "    df3=productHunt(company,keywords)\n",
    "    df4=ventureBeat(company,keywords)\n",
    "    df=pd.concat([df1,df2,df3,df4])\n",
    "    df.to_excel(writer, sheet_name=company,index=False)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(221, b'Bye')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import smtplib\n",
    "import datetime\n",
    "import email\n",
    "# text\n",
    "from email.mime.text import MIMEText\n",
    "# image\n",
    "from email.mime.image import MIMEImage\n",
    "#assemble\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.header import Header\n",
    "mail_host = \"smtp.163.com\"\n",
    "# sender\n",
    "mail_sender = \"oceansCrawler@163.com\"\n",
    "\n",
    "mail_license = \"RJXQCJKXYJUCJERM\"\n",
    "# receiver\n",
    "mail_receivers = [\"haijiang@wustl.edu\"]\n",
    "mm = MIMEMultipart('related')\n",
    "# subject\n",
    "subject_content = \"\"\"Oceans Crawler \"\"\"+str(datetime.date.today())\n",
    "# sender\n",
    "mm[\"From\"] = \"Crawler Robot<oceansCrawler@163.com>\"\n",
    "# receiver\n",
    "mm[\"To\"] = \"receiver_1_name<haijiang@wustl.edu>\"\n",
    "# Subject\n",
    "mm[\"Subject\"] = Header(subject_content,'utf-8')\n",
    "# content\n",
    "body_content = \"\"\"Hello，attanched is the data crawled！\"\"\"\n",
    "# param1：contene，param2：text type，param3：code tyoe\n",
    "message_text = MIMEText(body_content,\"plain\",\"utf-8\")\n",
    "\n",
    "mm.attach(message_text)\n",
    "\n",
    "filename='Target Company Info-'+str(datetime.date.today())+'.xlsx'\n",
    "# create att\n",
    "atta = MIMEText(open(filename, 'rb').read(), 'base64', 'utf-8')\n",
    "# set attachment\n",
    "atta[\"Content-Disposition\"] = 'attachment; filename='+filename\n",
    "# add attachment\n",
    "mm.attach(atta)\n",
    "\n",
    "# create STMP object\n",
    "stp = smtplib.SMTP()\n",
    "# setter\n",
    "stp.connect(mail_host, 25)  \n",
    "\n",
    "# login email，parm1：email adderss，param：authorization code\n",
    "stp.login(mail_sender,mail_license)\n",
    "# send email，param1：sender address，param2：receiver address，parms3：change content datatyoe to str\n",
    "stp.sendmail(mail_sender, mail_receivers, mm.as_string())\n",
    "print(\"sent successfully\")\n",
    "# shut down SMTP object\n",
    "stp.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
